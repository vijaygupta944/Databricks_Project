{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9329d19c-84ef-479d-be29-20440bd40dfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Phase 2: Data Transformation & Integration (1â€“1.5 Hours) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cc2773c-a2ec-44bb-ad37-30ca23323d2c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Standardize"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, to_date, when\n",
    "\n",
    "# Load tables from default schema\n",
    "table_names = ['raw_accounts', 'raw_customers', 'raw_customers', 'raw_transactions']\n",
    "dfs = [spark.table(f'default.{name}') for name in table_names]\n",
    "\n",
    "# Standardize date formats to 'yyyy-MM-dd'\n",
    "date_columns = ['created_date', 'updated_date', 'transaction_date']\n",
    "for i, df in enumerate(dfs):\n",
    "    for date_col in date_columns:\n",
    "        if date_col in df.columns:\n",
    "            df = df.withColumn(\n",
    "                date_col,\n",
    "                to_date(col(date_col), 'yyyy-MM-dd')\n",
    "            )\n",
    "    dfs[i] = df\n",
    "\n",
    "# Standardize numeric types to DoubleType\n",
    "numeric_columns = ['amount', 'balance', 'income']\n",
    "for i, df in enumerate(dfs):\n",
    "    for num_col in numeric_columns:\n",
    "        if num_col in df.columns:\n",
    "            df = df.withColumn(\n",
    "                num_col,\n",
    "                col(num_col).cast('double')\n",
    "            )\n",
    "    dfs[i] = df\n",
    "\n",
    "# Standardize boolean fields to BooleanType\n",
    "boolean_columns = ['is_active', 'is_joint_account']\n",
    "for i, df in enumerate(dfs):\n",
    "    for bool_col in boolean_columns:\n",
    "        if bool_col in df.columns:\n",
    "            df = df.withColumn(\n",
    "                bool_col,\n",
    "                when(\n",
    "                    col(bool_col).cast('string').isin('true', 'True', '1'),\n",
    "                    True\n",
    "                ).otherwise(False)\n",
    "            )\n",
    "    dfs[i] = df\n",
    "\n",
    "for df in dfs:\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5c56b3d-dca9-48f9-ae6c-c705a88647d9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Handle invalid/empty values"
    }
   },
   "outputs": [],
   "source": [
    "# Handle invalid/empty values: drop rows with null or NaN in any column using SQL\n",
    "for i, table_name in enumerate(table_names):\n",
    "    # Drop rows with NULLs in any column\n",
    "    sql_query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM default.{table_name}\n",
    "        WHERE {\" AND \".join([f\"{col} IS NOT NULL\" for col in dfs[i].columns])}\n",
    "    \"\"\"\n",
    "    # Additionally, filter out NaN in numeric columns\n",
    "    for num_col in numeric_columns:\n",
    "        if num_col in dfs[i].columns:\n",
    "            sql_query = sql_query.replace(\n",
    "                \"WHERE\",\n",
    "                f\"WHERE ({num_col} = {num_col} OR {num_col} IS NULL) AND\"\n",
    "            )\n",
    "    dfs[i] = spark.sql(sql_query)\n",
    "\n",
    "for df in dfs:\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79c84fa8-4db1-4742-9fe7-cd88ec57f400",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Remove duplicates"
    }
   },
   "outputs": [],
   "source": [
    "# Remove duplicate rows from each DataFrame\n",
    "for i, df in enumerate(dfs):\n",
    "    dfs[i] = df.dropDuplicates()\n",
    "\n",
    "for df in dfs:\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ff00ed0-752e-4755-9249-b09b82a03a98",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Normalize phone numbers, PAN, Aadhar if necessary"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, upper, trim\n",
    "\n",
    "# Define normalization functions for phone, PAN, and Aadhar\n",
    "def normalize_columns(df):\n",
    "    # Normalize phone numbers: remove non-digit characters, keep last 10 digits\n",
    "    if 'phone_number' in df.columns:\n",
    "        df = df.withColumn(\n",
    "            'phone_number',\n",
    "            regexp_replace(col('phone_number'), r'\\D', '')\n",
    "        ).withColumn(\n",
    "            'phone_number',\n",
    "            col('phone_number').substr(-9, 10)\n",
    "        )\n",
    "    # Normalize PAN: uppercase, remove spaces, keep valid pattern\n",
    "    if 'pan' in df.columns:\n",
    "        df = df.withColumn(\n",
    "            'pan',\n",
    "            upper(trim(regexp_replace(col('pan'), r'[^A-Z0-9]', '')))\n",
    "        )\n",
    "    # Normalize Aadhar: remove non-digit characters, keep 12 digits\n",
    "    if 'aadhar' in df.columns:\n",
    "        df = df.withColumn(\n",
    "            'aadhar',\n",
    "            regexp_replace(col('aadhar'), r'\\D', '')\n",
    "        ).withColumn(\n",
    "            'aadhar',\n",
    "            col('aadhar').substr(1, 12)\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# Apply normalization to all DataFrames\n",
    "for i, df in enumerate(dfs):\n",
    "    dfs[i] = normalize_columns(df)\n",
    "\n",
    "for df in dfs:\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb422e33-2cf5-4ba4-8501-36f12fc41812",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Join datasets logically"
    }
   },
   "outputs": [],
   "source": [
    "# Logical joins between standardized DataFrames\n",
    "accounts_df, customers_df, branches_df, transactions_df = dfs\n",
    "\n",
    "# Join accounts with customers on customer_id\n",
    "accounts_customers_df = accounts_df.join(\n",
    "    customers_df,\n",
    "    accounts_df.customer_id == customers_df.customer_id,\n",
    "    'inner'\n",
    ")\n",
    "\n",
    "# Join transactions with accounts on account_id\n",
    "accounts_customers_transactions_df = accounts_customers_df.join(\n",
    "    transactions_df,\n",
    "    col('account_id') == col('account_id'),\n",
    "    'inner'\n",
    ")\n",
    "\n",
    "# Rename duplicate columns by appending '_1' to the duplicate\n",
    "from collections import Counter\n",
    "\n",
    "cols = accounts_customers_transactions_df.columns\n",
    "col_counts = Counter(cols)\n",
    "new_cols = []\n",
    "seen = {}\n",
    "\n",
    "for col_name in cols:\n",
    "    if col_counts[col_name] > 1:\n",
    "        if col_name not in seen:\n",
    "            new_cols.append(col_name)\n",
    "            seen[col_name] = 1\n",
    "        else:\n",
    "            new_cols.append(f\"{col_name}_1\")\n",
    "            seen[col_name] += 1\n",
    "    else:\n",
    "        new_cols.append(col_name)\n",
    "\n",
    "accounts_customers_transactions_df = accounts_customers_transactions_df.toDF(*new_cols)\n",
    "\n",
    "display(accounts_customers_transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c52030-e87d-4e54-ae70-98d5a4d1ae4d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create enriched curated tables"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# Write cleaned DataFrames to Delta tables\n",
    "accounts_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_accounts\")\n",
    "customers_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_customers\")\n",
    "branches_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_branches\")\n",
    "transactions_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26957028-965d-4a76-8420-097de9a1bac8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create enriched curated Fact table"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Remove duplicate 'customer_id' column before saving\n",
    "if \"customer_id_1\" in accounts_customers_transactions_df.columns:\n",
    "    accounts_customers_transactions_df_clean = accounts_customers_transactions_df.drop(\"customer_id_1\")\n",
    "else:\n",
    "    accounts_customers_transactions_df_clean = accounts_customers_transactions_df\n",
    "\n",
    "accounts_customers_transactions_df_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_customer_account_txn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "707c02dc-8c1c-4e14-b339-6280801ae775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from silver_customer_account_txn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3ba5872-b7c2-4014-8351-166c10992748",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4881359310543887,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
