{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8bbf3eb-d083-4920-b3a9-55f4b43b868c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Phase 1: Metadata-Driven Ingestion (1â€“1.5 Hours) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2032d627-80d1-4778-9707-7a5c8d7f0dc4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define control table data for 4 files from the volume"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define control table data for 4 files from the volume\n",
    "control_data = [\n",
    "    {\n",
    "        \"file_path\": \"/Volumes/databricks_catalog/default/databricks_project_volume/accounts.json\",\n",
    "        \"table_name\": \"accounts\",\n",
    "        \"schema_info\": \"account_id STRING, customer_id STRING, account_type STRING, account_number STRING, branch_code STRING, opening_date DATE, account_status STRING, current_balance FLOAT, available_balance FLOAT, interest_rate FLOAT, minimum_balance FLOAT, overdraft_limit FLOAT, last_transaction_date DATE, is_joint_account BOOLEAN, nominee_name STRING, nominee_relationship STRING, created_timestamp TIMESTAMP\",\n",
    "        \"target_path\": \"/Volumes/databricks_catalog/default/databricks_project_volume/accounts_data\"\n",
    "    },\n",
    "    {\n",
    "        \"file_path\": \"/Volumes/databricks_catalog/default/databricks_project_volume/branches.json\",\n",
    "        \"table_name\": \"branches\",\n",
    "        \"schema_info\": \"branch_code STRING, branch_name STRING, branch_type STRING, ifsc_code STRING, micr_code STRING, street_address STRING, city STRING, state STRING, pincode STRING, landmark STRING, phone_numbers ARRAY<STRING>, email STRING, fax STRING, opening_time STRING, closing_time STRING, working_days STRING, atm_available BOOLEAN, parking_available BOOLEAN, wheelchair_accessible BOOLEAN, branch_manager STRING, total_employees INT, customer_service_officers INT, services_offered ARRAY<STRING>, region STRING, establishment_date DATE, license_number STRING, is_active BOOLEAN, last_audit_date DATE, compliance_score FLOAT, created_timestamp TIMESTAMP, last_updated TIMESTAMP\",\n",
    "        \"target_path\": \"/Volumes/databricks_catalog/default/databricks_project_volume/branches_data\"\n",
    "    },\n",
    "    {\n",
    "        \"file_path\": \"/Volumes/databricks_catalog/default/databricks_project_volume/customers.csv\",\n",
    "        \"table_name\": \"customers\",\n",
    "        \"schema_info\": \"customer_id STRING, first_name STRING, last_name STRING, email STRING, phone STRING, date_of_birth DATE, gender STRING, annual_income FLOAT, pan_number STRING, aadhar_number STRING, city STRING, state STRING, pincode STRING, customer_since DATE, kyc_status STRING, credit_score INT, risk_category STRING, is_active BOOLEAN\",\n",
    "        \"target_path\": \"/Volumes/databricks_catalog/default/databricks_project_volume/customers_data\"\n",
    "    },\n",
    "    {\n",
    "        \"file_path\": \"/Volumes/databricks_catalog/default/databricks_project_volume/transactions.csv\",\n",
    "        \"table_name\": \"transactions\",\n",
    "        \"schema_info\": \"transaction_id STRING, from_account_id STRING, to_account_id STRING, transaction_type STRING, amount FLOAT, transaction_date DATE, transaction_timestamp TIMESTAMP, channel STRING, merchant_name STRING, merchant_category STRING, description STRING, reference_number STRING, status STRING, currency STRING, exchange_rate FLOAT, fee_amount FLOAT, location_city STRING, location_state STRING, device_type STRING, ip_address STRING\",\n",
    "        \"target_path\": \"/Volumes/databricks_catalog/default/databricks_project_volume/transactions_data\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create DataFrame with simple schema_info as DDL string\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "df = spark.createDataFrame([Row(**item) for item in control_data], schema=StructType([\n",
    "    StructField('file_path', StringType(), True),\n",
    "    StructField('table_name', StringType(), True),\n",
    "    StructField('schema_info', StringType(), True),\n",
    "    StructField('target_path', StringType(), True)\n",
    "]))\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"default.control_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bdfe1ca-2cc8-4a5f-9fb6-a32b6300a074",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display of Control Table"
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26eb2920-dd43-4522-b530-7cf4f7714636",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read control table"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Read control table\n",
    "control_df = spark.table(\"default.control_table\")\n",
    "\n",
    "from pyspark.sql.types import _parse_datatype_string\n",
    "\n",
    "def get_default_value(data_type):\n",
    "    dtype = str(data_type)\n",
    "    if dtype == \"StringType\":\n",
    "        return \"\"\n",
    "    elif dtype in [\"IntegerType\", \"LongType\", \"ShortType\", \"ByteType\"]:\n",
    "        return 0\n",
    "    elif dtype in [\"FloatType\", \"DoubleType\", \"DecimalType\"]:\n",
    "        return 0.0\n",
    "    elif dtype == \"BooleanType\":\n",
    "        return False\n",
    "    elif dtype == \"DateType\":\n",
    "        return \"1970-01-01\"\n",
    "    elif dtype == \"TimestampType\":\n",
    "        return \"1970-01-01 00:00:00\"\n",
    "    else:\n",
    "        return None  # Only allow types supported by fillna\n",
    "\n",
    "for row in control_df.collect():\n",
    "    file_path = row['file_path']\n",
    "    table_name = row['table_name']\n",
    "    schema_info = row['schema_info']\n",
    "    target_path = row['target_path']\n",
    "    \n",
    "    # Only process CSV files\n",
    "    if file_path.endswith('.csv'):\n",
    "        schema = _parse_datatype_string(schema_info)\n",
    "        \n",
    "        df = (\n",
    "            spark.read\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"mode\", \"PERMISSIVE\")\n",
    "            .option(\"inferSchema\", \"false\")\n",
    "            .schema(schema)\n",
    "            .csv(file_path)\n",
    "        )\n",
    "        \n",
    "        # Only include supported types in fill_dict\n",
    "        fill_dict = {\n",
    "            field.name: get_default_value(field.dataType)\n",
    "            for field in schema.fields\n",
    "            if get_default_value(field.dataType) is not None\n",
    "        }\n",
    "        if fill_dict:\n",
    "            df_clean = df.fillna(fill_dict)\n",
    "        else:\n",
    "            df_clean = df\n",
    "        \n",
    "        # Validate schema: check column names\n",
    "        if set(df_clean.columns) == set([field.name for field in schema.fields]):\n",
    "            df_clean.write.mode(\"overwrite\").saveAsTable(f\"default.raw_{table_name}\")\n",
    "        else:\n",
    "            print(f\"Schema mismatch for {table_name}: skipping ingestion.\")\n",
    "\n",
    "metadata_df = spark.createDataFrame(\n",
    "    [row.asDict() for row in control_df.collect()]\n",
    ")\n",
    "metadata_df.write.mode(\"overwrite\").saveAsTable(\"default.metadata_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad8a41e4-d4f1-49d9-9c85-7387c3fb06c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "metadata_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "344eb397-c66a-4f5e-9a67-102f328b2959",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read and write the acounts file to raw_accounts table"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_accounts = (\n",
    "    spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .json(\"/Volumes/databricks_catalog/default/databricks_project_volume/accounts.json\")\n",
    ")\n",
    "\n",
    "df_accounts.write.mode(\"overwrite\").saveAsTable(\"default.raw_accounts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06dc19de-30b5-4764-9233-292eacfc9ad8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read and write the branches file to raw_branches table"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_branches = (\n",
    "    spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .json(\"/Volumes/databricks_catalog/default/databricks_project_volume/branches.json\")\n",
    ")\n",
    "\n",
    "df_branches.write.mode(\"overwrite\").saveAsTable(\"default.raw_branches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4084f2d-3782-44bc-8f81-016de20d695c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read and write the customers file to raw_customers table"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_customers = (\n",
    "    spark.read\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(\"/Volumes/databricks_catalog/default/databricks_project_volume/customers.csv\")\n",
    ")\n",
    "\n",
    "\n",
    "df_customers.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"default.raw_customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "473f5d02-6e25-4b64-9f49-5d0166d26cc0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read and write the transactionsfile to raw_transactions table"
    }
   },
   "outputs": [],
   "source": [
    "df_transactions = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(\"/Volumes/databricks_catalog/default/databricks_project_volume/transactions.csv\")\n",
    ")\n",
    "df_transactions.write.mode(\"overwrite\").saveAsTable(\"default.raw_transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecb73fae-43a8-427c-88e6-f349cba191b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingestion.",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
